sum(fcast$upper>testing$visitsTumblr)
sum(fcast$upper>testing$visitsTumblr)/dim(testing)[1]
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level = 95, h = dim(testing)[1])
sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) /
dim(testing)[1]
sum(testing$visitsTumblr < fcast$upper) /
dim(testing)[1]
mod_ts
summary(mod_ts)
plot
plot(mod_ts)
plot(testing$visitsTumblr)
plot(ts(testing$visitsTumblr))
plot(ts(dat$visitsTumblr))
lines(fcast$upper)
str(Fcast)
str(fcast)
fcast$upper
sum(fcast$upper>testing$visitsTumblr)/dim(testing)[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testin
testing = concrete[-inTrain,]
dim(testing)
dim(training)
head(training)
install.packages("e1071")
library(e1071)
install.packages("e1071")
library(e1071)
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
library(e1071)
library(caret)
?e1071
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testin
testing = concrete[ -inTrain,]
set.seed(325)
mdlFit <- svm(CompressiveStrength ~., data = training)
predictions <- predict(mdlFit, newdata = mdlFit)
predictions <- predict(mdlFit, newdata = testing)
plot(predictions)
mdlFit
confusionMatrix(testing$CompressiveStrength, predictions)
str(testing$CompressiveStrength)
str(testing$predictions)
str(predictions)
str(as.numeric(predictions)0
str(as.numeric(predictions))
confusionMatrix(testing$CompressiveStrength, as.numeric(predictions))
rmse(testing$CompressiveStrength, predictions)
RMSE = function(m, o){
sqrt(mean((m - o)^2))
}
RMSE(testing$CompressiveStrength, predictions)
accuracy(testing$CompressiveStrength, predictions)
?accuracy
install.packages("accuracy")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[inTrain, ]
testing = concrete[-inTrain, ]
set.seed(325)
library(e1071)
mod_svm <- svm(CompressiveStrength ~ ., data = training)
pred_svm <- predict(mod_svm, testing)
accuracy(pred_svm, testing$CompressiveStrength)
RMSE(pred_svm, testing$CompressiveStrength)
install.packages("accuracy")
install.packages("rmse")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
dim(training)
View(training)
names(training)
str(training)
df <- training
seed(3234)
set.seed(3234)
inTrain <- createDataPartition(y=df$classe, p=0.60, list=FALSE)
training <- df[inTrain,]
testing <- df[-inTrain,]
table(training$classe)
?all
apply(training, 2, function(x) all(is.na(x)))
table(training$amplitude_pitch_arm)
table(training$amplitude_yaw_arm)
sum(is.na(training$amplitude_yaw_arm))
dim(training$amplitude_yaw_arm)
dim(training)
sample(1:10)
sample(1:3)
sample(1:3,1)
sample(1:10, 3)
dim(training)
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
str(nacnt)
nacnt!=0
?whcih
?which
which(nacnt!=0)
head(nacnt)
nacnt[which(nacnt!=0)]
View(nacnt[which(nacnt!=0)])
View(nacnt)
dim(training)
sun(is.na(training$avg_roll_arm))
sum(is.na(training$avg_roll_arm))
sum(is.na(training$avg_roll_arm)) - 11776
View(training[which(training$amplitude_pitch_belt!=NA),])
which(training$amplitude_pitch_belt!=NA)
which(training$amplitude_pitch_belt!="NA")
View(training[which(training$amplitude_pitch_belt!="NA"),])
training.nonNA = training[which(training$amplitude_pitch_belt!="NA"),]
table(training.nonNA$user_name)
names(training)
dim(training.nonNA)
df1 <- data.frame(X, username, data = training)
dim(training)
getwd()
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
df <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
df.20tests <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
set.seed(3234)
inTrain <- createDataPartition(y=df$classe, p=0.80, list=FALSE)
training <- df[inTrain,]
testing <- df[-inTrain,]
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
train.filtered <- training[,-which(nacnt!=0)]
nzv <- nearZeroVar(train.filtered, saveMetrics = T)
head(nzv)
nzv[where(nzv$nzv==TRUE),]
nzv[which(nzv$nzv==TRUE),]
?nearZeroVar
table[train.filtered$new_window]
table(train.filtered$new_window)
head(nacnt)
head(which(nacnt!=0))
head(nacnt(which(nacnt!=0))
)
head(nacnt(which(nacnt!=0)))
head(nacnt[which(nacnt!=0]))
head(nacnt[which(nacnt!=0)]))
head(nacnt[,which(nacnt!=0)]))
table(nacnt)
nzv[which(nzv$nzv==TRUE),]
table(train.filtered$skewness_yaw_arm)
table(train.filtered$kurtosis_picth_dumbbell)
View(train.filtered)
emptycnt <- apply(train.filtered, 2, function(x) sum(x==""))
head(emptycnt)
View(training[,which(emptycnt==15377)])
head(emptycnt[emptycnt>0])
table(emptycnt[emptycnt>0])
train.filtered <- train.filtered[,-which(emptycnt>0)]
dim(train.filtered)
dim(training)
# Check for near zero variance variables
nzv <- nearZeroVar(train.filtered, saveMetrics = T)
nzv
nzv[nzv$nzv==TRUE,]
table(train.filtered$new_window)
View(training[training$new_window=="yes",])
View(train.filtered[train.filtered$new_window=="yes",])
trainCor <- cor(train.filtered)
?cor
str(training.filtered)
str(train.filtered)
# Check for highly correlated predictors
trainCor <- cor(train.filtered)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
library(dplyr)
# Check for highly correlated predictors
train.numeric.only <- train.filtered %>% select_if(x, is.numeric)
# Check for highly correlated predictors
train.numeric.only <- select_if(train.filtered, is.numeric)
trainCor <- cor(train.numeric.only)
highCorVars <- findCorrelation(trainCor, cutoff = .75)
dim(highCorVars)
train.filtered <- train.filtered[,-highCorVar]
highCorVars
trainCor
trainCor[highCorVars, highCorVars]
View(trainCor[highCorVars, highCorVars])
length(highCorVars)
# Fit random forest model to data first
mdl.rf <- train(classe ~., method = "rf", data = train.filtered)
summary(mdl.rf$finalModel)
mdl.rf$finalModel
pred <- predict(mdl.rf, testing)
testing$predRight <- pred == testing$classe
table(pred, testing$classe)
mdl.rf
confusionMatrix(pred, testing$classe)
str(pred)
str(testing$classe)
confusionMatrix(pred, as.factor(testing$classe))
hist(training$classe)
plot(training$classe)
str(training$classe)
hist(as.factor(training$classe))
plot(as.factor(training$classe))
qplot(as.factor(training$classe))
?qplot
qplot(as.factor(training$classe), main = "Distribution of classe Variable for Training Set", xlab = "classe", ylab = "Frequency")
str(nacnt)
nacnt[nacnt!=0,]
nacnt[which(nacnt!=0),]
str(which(nacnt!=0))
nacnt[which(nacnt!=0),]
str(which(nacnt!=0))
which(nacnt!=0)
str(nacnt)
str(which(nacnt!=0))
str(nacnt[which(nacnt!=0)])
plot(nacnt[which(nacnt!=0)])
nzv
str(train.filtered$new_window)
table(train.filtered$new_window)
nzv[nzv$nzv==TRUE,]
# Get subset of training data with new_window == "yes"
train.nw.yes <- train.filtered[train.filtered$new_window=="yes",]
nacnt.nw <- apply(train.nw.yes, 2, function(x) sum(is.na(x)))
nacnt.nw
dim(train.nw.yes)
table(train.filtered$new_window)
which(nacnt.nw!=0)
emptycnt.nw <- apply(train.nw.yes, 2, function(x) sum(x==""))
which(emptycnt.nw!=0)
View(train.nw.yes)
train.nw.no <- train.filtered[train.filtered$new_window=="no",]
nacnt.nw.no <- apply(train.nw.yes, 2, function(x) sum(is.na(x)))
emptycnt.nw.no <- apply(train.nw.yes, 2, function(x) sum(x==""))
nacnt.nw.no
nacnt.nw.no <- apply(train.nw.no, 2, function(x) sum(is.na(x)))
emptycnt.nw.no <- apply(train.nw.no, 2, function(x) sum(x==""))
nacnt.nw.no
train.nw.no <- train.filtered[train.filtered$new_window=="no",]
nacnt.nw.no <- apply(train.nw.no, 2, function(x) sum(is.na(x)))
emptycnt.nw.no <- apply(train.nw.no, 2, function(x) sum(x==""))
nacnt.nw.no
emptycnt.nw.no
View(train.nw.no)
train.nw.yes <- training[train.filtered$new_window=="yes",]
train.nw.no <- training[train.filtered$new_window=="no",]
nacnt.nw.no <- apply(train.nw.no, 2, function(x) sum(is.na(x)))
emptycnt.nw.no <- apply(train.nw.no, 2, function(x) sum(x==""))
nacnt.nw.no
nacnt.nw.no[nacnt.nw.no!=0]
table(nacnt.nw.no[nacnt.nw.no!=0])
table(train.filtered$new_window)
table(emptycnt.nw.no[emptycnt.nw.no!=0])
train.nw.yes <- training[train.filtered$new_window=="yes",]
nacnt.nw.yes <- apply(train.nw.yes, 2, function(x) sum(is.na(x)))
emptycnt.nw.yes <- apply(train.nw.yes, 2, function(x) sum(x==""))
train.nw.yes <- training[train.filtered$new_window=="yes",]
nacnt.nw.yes <- apply(train.nw.yes, 2, function(x) sum(is.na(x)))
emptycnt.nw.yes <- apply(train.nw.yes, 2, function(x) sum(x==""))
table(nacnt.nw.yes[nacnt.nw.yes!=0])
table(emptycnt.nw.yes[emptycnt.nw.yes!=0])
nacnt.nw.yes
sum[nacnt.nw.yes!=0]
sum[emptycnt.nw.yes!=0]
sum(nacnt.nw.yes!=0)
sum(emptycnt.nw.yes!=0)
mdl.rf
getTree(mdl.rf$finalModel, k=2)
install.packages("getTree")
install.packages("apex")
getTree(mdl.rf$finalModel, k=2)
remove.packages("apex")
library(randomForest)
getTree(mdl.rf$finalModel, k=2)
?getTree
getTree(mdl.rf$finalModel, k=1)
# Plot the historgram of outcome variable
qplot(as.factor(training$classe), main = "Distribution of Outcome Variable of Training Set", xlab = "classe", ylab = "Frequency")
qplot(as.factor(training$classe), main = "Distribution of Outcome Variable of Training Set", xlab = "classe", ylab = "Frequency")
?qplot
qplot(as.factor(training$classe), main = "Distribution of Outcome Variable of Training Set", xlab = "classe", ylab = "Frequency")
table(train.filtered$new_window)
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
train.filtered <- training[,-which(nacnt!=0)]
emptycnt <- apply(train.filtered, 2, function(x) sum(x==""))
train.filtered <- train.filtered[,-which(emptycnt>0)]
dim(train.filtered)
nacnt <- apply(training, 2, function(x) sum(is.na(x) + sum(x=="")))
train.filtered <- training[,-which(nacnt!=0)]
dim(train.filtered)
dim(training)
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
train.filtered <- training[,-which(nacnt!=0)]
emptycnt <- apply(train.filtered, 2, function(x) sum(x==""))
train.filtered <- train.filtered[,-which(emptycnt>0)]
# Remove records with new_window == "yes"
train.filtered <- train.filtered[,-c("new_window")]
# Remove records with new_window == "yes"
train.filtered <- train.filtered[,-c(new_window)]
# Remove rows with new_window == "yes"
train.filtered <- train.filtered[-which(train.filtered$new_window=="yes")]
dim(train.filtered)
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
train.filtered <- training[,-which(nacnt!=0)]
emptycnt <- apply(train.filtered, 2, function(x) sum(x==""))
train.filtered <- train.filtered[,-which(emptycnt>0)]
# Remove rows with new_window == "yes"
train.filtered <- train.filtered[-which(train.filtered$new_window=="yes"),
]
dim(train.filtered)
# Remove new_window variable
train.filtered <- subset(train.filtered, select = -c(new_window))
dim(train.filtered)
train.numeric.only <- select_if(train.filtered, is.numeric)
trainCor <- cor(train.numeric.only)
highCorVars <- findCorrelation(trainCor, cutoff = .75)
length(highCorVars)
confusionMatrix(pred, as.factor(testing$classe))
mdl.rf
?rf
?randomForest
mdl.rf$modelInfo
mdl.rf$method
mdl.rf$preProcess
mdl.rf$finalModel
mdl.rf
pred <- predict(mdl.rf, testing)
confusionMatrix(pred, as.factor(testing$classe))
quiz.predictions <- predict(mdl.rf, df.20tests)
quiz.predictions
table(predictions)
table(pred)
quiz.predictions <- predict(mdl.rf, newdata = df.20tests)
quiz.predictions
pred <- predict(mdl.rf, newdata = testing)
confusionMatrix(pred, as.factor(testing$classe))
length(quiz.predictions)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
library(dplyr)
dim(df.20tests)
library(caret)
library(rattle)
TrainData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
dim(TrainData)
TestData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
dim(TestData)
# Here we get the indexes of the columns having at least 90% of NA or blank values on the training dataset
indColToRemove <- which(colSums(is.na(TrainData) |TrainData=="")>0.9*dim(TrainData)[1])
TrainDataClean <- TrainData[,-indColToRemove]
TrainDataClean <- TrainDataClean[,-c(1:7)]
dim(TrainDataClean)
# We do the same for the test set
indColToRemove <- which(colSums(is.na(TestData) |TestData=="")>0.9*dim(TestData)[1])
TestDataClean <- TestData[,-indColToRemove]
TestDataClean <- TestDataClean[,-1]
dim(TestDataClean)
# Here we create a partition of the traning data set
set.seed(12345)
inTrain1 <- createDataPartition(TrainDataClean$classe, p=0.75, list=FALSE)
Train1 <- TrainDataClean[inTrain1,]
Test1 <- TrainDataClean[-inTrain1,]
dim(Train1)
trControl <- trainControl(method="cv", number=5)
model_RF <- train(classe~., data=Train1, method="rf", trControl=trControl, verbose=FALSE)
print(model_RF)
plot(model_RF,main="Accuracy of Random forest model by number of predictors")
trainpred <- predict(model_RF,newdata=Test1)
confMatRF <- confusionMatrix(Test1$classe,trainpred)
str(Test1$classe)
str(trainpred)
confMatRF <- confusionMatrix(as.factor(Test1$classe),trainpred)
confMatRF$table
model_RF
FinalTestPred <- predict(model_RF,newdata=TestDataClean)
FinalTestPred
dim(Train1)
dim(Test1)
dim(TestData)
model_RF_bootstrap <- train(classe~., data=Train1, method="rf", verbose=FALSE)
predict(model_RF_bootstrap, newdata = TestDataClean)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
library(dplyr)
df.20tests <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
predict(model_RF_bootstrap, newdata = df.20tests)
predict(model_RF, newdata = df.20tests)
df <- TrainData
# Split available data into 80% training and 20% testing sets
set.seed(3234)
inTrain <- createDataPartition(y=df$classe, p=0.80, list=FALSE)
training <- df[inTrain,]
testing <- df[-inTrain,]
# Remove variables with primarily NA and empty values
nacnt <- apply(training, 2, function(x) sum(is.na(x)))
train.filtered <- training[,-which(nacnt!=0)]
emptycnt <- apply(train.filtered, 2, function(x) sum(x==""))
train.filtered <- train.filtered[,-which(emptycnt>0)]
# Remove rows with new_window == "yes"
train.filtered <- train.filtered[-which(train.filtered$new_window=="yes"),]
# Remove new_window variable
train.filtered <- subset(train.filtered, select = -c(new_window))
dim(train.filtered)
dim(Train1)
head(train1)
str(Train1)
str(train.filtered)
str(train.filtered$classe)
table(train.filtered$classe)
train.filtered2 <- train.filtered[-1:6,]
train.filtered2 <- train.filtered[-c(1:6),]
str(train.filtered$classe)
table(train.filtered)
dim(train.filtered2)
train.filtered2 <- train.filtered[,-c(1:6)]
dim(train.filtered2)
str(train.filtered2)
names(Train1)
str(train.filtered2)
model_RF_bootstrap_train.filtered <- train(classe~., data=train.filtered, method="rf", verbose=FALSE)
model_RF_bootstrap_train.filtered2 <- train(classe~., data=train.filtered2, method="rf", verbose=FALSE)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
predict(model_RF_bootstrap_train.filtered, newdata = testing)
predict(model_RF_bootstrap_train.filtered, newdata = df.20tests)
predict(model_RF_bootstrap_train.filtered2, newdata = df.20tests)
dim(train.filtered2)
dim(train.filtered)
head(train.filtered[,1:6])
head(train.filtered[,3:6])
model_RF_bootstrap_train.filtered
model_RF_bootstrap_train.filtered2
model_RF_bootstrap_train.filtered$finalModel
model_RF_bootstrap_train.filtered2$finalModel
varImp(model_RF_bootstrap_train.filtered)
plot(train.filtered$X, train.filtered$classe)
plot(train.filtered$classe ~ train.filtered$X)
boxplot(train.filtered$classe ~ train.filtered$X)
plot(x = train.filtered$X, y= train.filtered$classe)
plot(x = as.numeric(train.filtered$X), y= as.factor(train.filtered$classe))
names(train.filtered)
varImp(model_RF_bootstrap_train.filtered2)
plot(x = as.numeric(train.filtered$user_name), y= as.factor(train.filtered$classe))
plot(x = train.filtered$user_name, y= as.factor(train.filtered$classe))
table(train.filtered$user_name, train.filtered$classe)
install.packages(c("miniUI", "shiny", "googleVis", "plotly", "tidyr", "leaflet"))
library(leaflet)
my_map<- leaflet() %>%
addTiles()
my_map
my_map
my_map
setwd("~/GitHub/DevDataProds/Week 2 Assignment")
?read.csv
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
data <- read.csv("dc_restaurants_top10.csv")
str(data)
?addTiles
df <- data.frame(lat = data$Lat, lng = data$Long)
my_map<- df %>%
leaflet() %>%
addTiles
my_map
df <- data.frame(lat = data$Lat, lng = data$Long)
my_map<- df %>%
leaflet() %>%
addTiles() %>%
addMarkers()
my_map
"a" + " b"
"a" + & b"
"a" & b"
paste("a", "b")
paste("a", "b", sep = "")
my_map<- leaflet() %>%
addTiles() %>%
addMarkers(lat = data$Lat, lng = data$Long)
my_map
data$popup <- paste("<a href='",data$Link, "'>",data$Name,"</a>",sep="")
my_map<- leaflet() %>%
addTiles() %>%
addMarkers(lat = data$Lat, lng = data$Long, popup = data$popup)
my_map
?makeIcon
rest.icon <- makeIcon(
iconUrl = "restaurant_icon.png"
)
rest.icon
